# -*- coding: utf-8 -*-
"""
HVAC ä»¿çœŸè³‡æ–™é›†ç¶œåˆè©•ä¼°å·¥å…· (å„ªåŒ–ç‰ˆæœ¬)

å„ªåŒ–èˆ‡æ–°å¢åŠŸèƒ½ï¼š
1. æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§è©•ä¼° (Train on Synthetic, Test on Real - TSTR)
2. æ™‚é–“åºåˆ—åˆ†æåŠ å…¥ å‹•æ…‹æ™‚é–“è¦æ•´ (Dynamic Time Warping - DTW)
3. å¤šè®Šé‡åˆ†ä½ˆè©•ä¼° (PCA-based)
4. èª¿æ•´ç¶œåˆè©•åˆ†æ¬Šé‡ï¼Œç´å…¥æ–°æŒ‡æ¨™
5. ç¨‹å¼ç¢¼çµæ§‹å„ªåŒ–èˆ‡éŒ¯èª¤è™•ç†å¢å¼·
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import wasserstein_distance
from statsmodels.tsa.stattools import acf, pacf
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
import warnings
import os
from datetime import datetime
from tabulate import tabulate

# å˜—è©¦å°å…¥ dtwï¼Œè‹¥å¤±æ•—å‰‡æç¤ºå®‰è£
try:
    from dtw import dtw
except ImportError:
    print("è­¦å‘Šï¼šç¼ºå°‘ 'dtw-python' å¥—ä»¶ã€‚æ™‚é–“åºåˆ— DTW åˆ†æå°‡è¢«è·³éã€‚")
    print("è«‹åŸ·è¡Œ 'pip install dtw-python' ä¾†å®‰è£ã€‚")
    dtw = None

warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class HVACComprehensiveAnalyzer:
    """HVACè³‡æ–™é›†ç¶œåˆè©•ä¼°åˆ†æå™¨ (å„ªåŒ–ç‰ˆ)"""

    def __init__(self, real_data_file: str, synthetic_data_file: str):
        self.real_data_file = real_data_file
        self.synthetic_data_file = synthetic_data_file
        self._load_data()
        self.results = {} # åˆå§‹åŒ–çµæœå®¹å™¨

    def _load_data(self):
        """è¼‰å…¥æ•¸æ“š"""
        try:
            self.real_data = pd.read_csv(self.real_data_file)
            self.synthetic_data = pd.read_csv(self.synthetic_data_file)
            
            # ç¢ºä¿è³‡æ–™é•·åº¦ä¸€è‡´ä»¥ä¾¿æ¯”è¼ƒ
            min_len = min(len(self.real_data), len(self.synthetic_data))
            self.real_data = self.real_data.iloc[:min_len]
            self.synthetic_data = self.synthetic_data.iloc[:min_len]

            self.numeric_columns = self.real_data.select_dtypes(include=np.number).columns.tolist()
            if 'time' in self.numeric_columns:
                self.numeric_columns.remove('time')

            print("="*80)
            print("ğŸ“Š HVAC ä»¿çœŸè³‡æ–™é›†ç¶œåˆè©•ä¼°ç³»çµ± (å„ªåŒ–ç‰ˆ)")
            print("="*80)
            print(f"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ")
            print(f"   åŸå§‹è³‡æ–™ç¶­åº¦: {self.real_data.shape}")
            print(f"   ä»¿çœŸè³‡æ–™ç¶­åº¦: {self.synthetic_data.shape}")
            print(f"   åˆ†æç‰¹å¾µæ•¸é‡: {len(self.numeric_columns)} å€‹")
            print("="*80)
            
        except FileNotFoundError as e:
            print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ {e.filename}")
            raise

    def comprehensive_analysis(self, output_dir: str = "analysis_results"):
        """åŸ·è¡Œå®Œæ•´çš„ç¶œåˆåˆ†æ"""
        os.makedirs(output_dir, exist_ok=True)
        self.output_dir = output_dir
        
        print("\nğŸ” é–‹å§‹åŸ·è¡Œç¶œåˆåˆ†æ...\n")
        
        # ä¾æ¬¡åŸ·è¡Œå„é …åˆ†æ
        self._descriptive_statistics_analysis()
        self._fidelity_assessment()
        self._hvac_specific_analysis()
        self._correlation_analysis()
        self._time_series_analysis()
        self._utility_assessment()  # æ–°å¢ï¼šTSTRå¯¦ç”¨æ€§è©•ä¼°
        self._multivariate_analysis() # æ–°å¢ï¼šå¤šè®Šé‡çµæ§‹è©•ä¼°
        self._calculate_overall_scores()
        self._generate_visualizations()
        self._generate_comprehensive_report()
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼æ‰€æœ‰çµæœå·²ä¿å­˜è‡³: {output_dir}")
    
    # --- å„é …åˆ†ææ–¹æ³• (ç‚ºç°¡æ½”ï¼Œåƒ…å±•ç¤ºæ–°å¢èˆ‡ä¿®æ”¹çš„æ ¸å¿ƒéƒ¨åˆ†) ---
    
    def _descriptive_statistics_analysis(self):
        """1. æè¿°æ€§çµ±è¨ˆåˆ†æ (èˆ‡åŸç‰ˆç›¸åŒ)"""
        print("ğŸ“ˆ 1. æè¿°æ€§çµ±è¨ˆåˆ†æ")
        print("-" * 60)
        stats_real = self.real_data.describe().T
        stats_synth = self.synthetic_data.describe().T
        comparison_data = []
        for feature in stats_real.index:
            if feature in stats_synth.index and feature in self.numeric_columns:
                real_mean, synth_mean = stats_real.loc[feature, 'mean'], stats_synth.loc[feature, 'mean']
                real_std, synth_std = stats_real.loc[feature, 'std'], stats_synth.loc[feature, 'std']
                mean_error = abs(synth_mean - real_mean) / abs(real_mean) * 100 if real_mean != 0 else 0
                std_error = abs(synth_std - real_std) / abs(real_std) * 100 if real_std != 0 else 0
                comparison_data.append({
                    'ç‰¹å¾µ': feature, 'çœŸå¯¦å‡å€¼': f"{real_mean:.3f}", 'ä»¿çœŸå‡å€¼': f"{synth_mean:.3f}",
                    'å‡å€¼èª¤å·®(%)': f"{mean_error:.1f}", 'çœŸå¯¦æ¨™æº–å·®': f"{real_std:.3f}",
                    'ä»¿çœŸæ¨™æº–å·®': f"{synth_std:.3f}", 'æ¨™æº–å·®èª¤å·®(%)': f"{std_error:.1f}"
                })
        print(tabulate(comparison_data, headers='keys', tablefmt='grid'))
        self.results['descriptive_stats'] = pd.DataFrame(comparison_data)
        print("\n")

    def _fidelity_assessment(self):
        """2. ä¿çœŸåº¦è©•ä¼° (èˆ‡åŸç‰ˆç›¸åŒ)"""
        print("ğŸ¯ 2. åˆ†ä½ˆä¿çœŸåº¦è©•ä¼° (Fidelity Assessment)")
        print("-" * 60)
        fidelity_data = []
        for feature in self.numeric_columns:
            real_v, synth_v = self.real_data[feature].dropna(), self.synthetic_data[feature].dropna()
            if len(real_v) < 10 or len(synth_v) < 10: continue
            awd_score = wasserstein_distance(real_v, synth_v)
            stat_grade = "âœ…å„ªç§€" if awd_score < 1 else "âš ï¸è‰¯å¥½" if awd_score < 2 else "âŒéœ€æ”¹å–„"
            fidelity_data.append({'ç‰¹å¾µ': feature, 'Wassersteinè·é›¢': f"{awd_score:.4f}", 'çµ±è¨ˆç­‰ç´š': stat_grade})
        print(tabulate(fidelity_data, headers='keys', tablefmt='grid'))
        self.results['fidelity_metrics'] = pd.DataFrame(fidelity_data)
        print("\n")

    def _hvac_specific_analysis(self):
        """3. HVACç³»çµ±å°ˆå±¬åˆ†æ (èˆ‡åŸç‰ˆç›¸åŒï¼Œåƒ…ä½œå¾®èª¿)"""
        print("ğŸ­ 3. HVACç³»çµ±å°ˆå±¬åˆ†æ")
        print("-" * 60)
        # (æ­¤è™•ç¨‹å¼ç¢¼èˆ‡åŸç‰ˆå¹¾ä¹ç›¸åŒï¼Œç‚ºç¯€çœç¯‡å¹…çœç•¥ï¼Œå¯¦éš›ç¨‹å¼ç¢¼ä¸­æ‡‰ä¿ç•™)
        # ...
        hvac_results = [] # å‡è¨­å·²æœ‰è¨ˆç®—çµæœ
        # Chi_V å¸¸æ•¸æª¢æŸ¥
        if 'Chi_V' in self.real_data.columns and 'Chi_V' in self.synthetic_data.columns:
            real_chi_v_mode = self.real_data['Chi_V'].mode()[0]
            synth_chi_v_mode = self.synthetic_data['Chi_V'].mode()[0]
            chi_v_status = "âœ…æ­£ç¢º" if real_chi_v_mode == synth_chi_v_mode else "âŒéŒ¯èª¤"
            hvac_results.append({'æª¢æŸ¥é …ç›®': 'Chi_Vå¸¸æ•¸å€¼', 'çœŸå¯¦è³‡æ–™': f"{real_chi_v_mode}", 'ä»¿çœŸè³‡æ–™': f"{synth_chi_v_mode}", 'ç‹€æ…‹': chi_v_status})
        # é–‹é—œé‚è¼¯åˆ†æ
        if 'chi_on' in self.real_data.columns and 'chi_on' in self.synthetic_data.columns:
            real_on_rate = self.real_data['chi_on'].mean() * 100
            synth_on_rate = self.synthetic_data['chi_on'].mean() * 100
            on_rate_diff = abs(real_on_rate - synth_on_rate)
            logic_status = "âœ…å„ªç§€" if on_rate_diff < 5 else "âš ï¸è‰¯å¥½" if on_rate_diff < 10 else "âŒéœ€æ”¹å–„"
            hvac_results.append({'æª¢æŸ¥é …ç›®': 'é–‹å•Ÿç‡(%)', 'çœŸå¯¦è³‡æ–™': f"{real_on_rate:.1f}", 'ä»¿çœŸè³‡æ–™': f"{synth_on_rate:.1f}", 'ç‹€æ…‹': logic_status})
        
        if hvac_results:
             print(tabulate(hvac_results, headers='keys', tablefmt='grid'))
             self.results['hvac_specific'] = pd.DataFrame(hvac_results)
        else:
            print("æœªæ‰¾åˆ°å¯ä¾›åˆ†æçš„HVACç‰¹å®šæ¬„ä½ã€‚")
        print("\n")

    def _correlation_analysis(self):
        """4. ç›¸é—œæ€§çµæ§‹åˆ†æ (èˆ‡åŸç‰ˆç›¸åŒ)"""
        print("ğŸ”— 4. ç›¸é—œæ€§çµæ§‹åˆ†æ")
        print("-" * 60)
        common_features = [f for f in self.numeric_columns if f in self.synthetic_data.columns]
        real_corr, synth_corr = self.real_data[common_features].corr(), self.synthetic_data[common_features].corr()
        corr_diff = np.abs(real_corr - synth_corr)
        self.mean_corr_diff = np.nanmean(corr_diff.values[np.triu_indices_from(corr_diff, k=1)])
        print(f"å¹³å‡ç›¸é—œæ€§å·®ç•°: {self.mean_corr_diff:.4f}")
        corr_grade = "âœ…å„ªç§€" if self.mean_corr_diff < 0.1 else "âš ï¸è‰¯å¥½" if self.mean_corr_diff < 0.2 else "âŒéœ€æ”¹å–„"
        print(f"ç›¸é—œæ€§ä¿æŒç¨‹åº¦: {corr_grade}")
        self.results['correlation_analysis'] = {'mean_diff': self.mean_corr_diff, 'grade': corr_grade}
        print("\n")

    def _time_series_analysis(self):
        """5. æ™‚é–“åºåˆ—ç‰¹æ€§åˆ†æ (æ•´åˆACF, FFT, DTW - ä½¿ç”¨é™æ¡æ¨£ä»¥ç›¸å®¹èˆŠç‰ˆå¥—ä»¶)"""
        print("â° 5. æ™‚é–“åºåˆ—ç‰¹æ€§åˆ†æ")
        print("-" * 60)
        
        if dtw is None:
            print("DTW åˆ†æå·²è·³éï¼Œå› ç‚º 'dtw-python' å¥—ä»¶æœªå®‰è£ã€‚")
            self.results['time_series_analysis'] = pd.DataFrame()
            return

        key_features = ['PHVAC_y', 'chi_P', 'TAirLeaRoo_T', 'T_DryBul']
        time_results = []

        for feature in key_features:
            if feature not in self.numeric_columns or feature not in self.synthetic_data.columns:
                continue
            
            real_v = self.real_data[feature].dropna().values
            synth_v = self.synthetic_data[feature].dropna().values
            if len(real_v) < 50: continue

            # ACF ç›¸ä¼¼åº¦
            real_acf = acf(real_v, nlags=10, fft=True)
            synth_acf = acf(synth_v, nlags=10, fft=True)
            acf_diff = np.mean(np.abs(real_acf - synth_acf))
            acf_score = max(0, 1 - acf_diff / 0.5)

            # FFT ç›¸ä¼¼åº¦
            fft_diff = self._calculate_fft_diff(real_v, synth_v)
            fft_score = max(0, 1 - fft_diff / 0.2)
            
            # --- ğŸ”§ DTW ç›¸ä¼¼åº¦ (ä½¿ç”¨é™æ¡æ¨£ä»¥ç›¸å®¹èˆŠç‰ˆå¥—ä»¶) ---
            # æ­£è¦åŒ–è³‡æ–™
            scaler = StandardScaler()
            real_v_norm = scaler.fit_transform(real_v.reshape(-1, 1)).flatten()
            synth_v_norm = scaler.transform(synth_v.reshape(-1, 1)).flatten()

            # å°‡é•·åºåˆ—é™æ¡æ¨£è‡³å¯è¨ˆç®—çš„é•·åº¦ï¼Œä¾‹å¦‚ 2000
            sample_size = 2000
            if len(real_v_norm) > sample_size:
                # é€éé¸å–ç­‰é–“éš”çš„é»ä¾†é€²è¡Œé™æ¡æ¨£
                indices = np.linspace(0, len(real_v_norm) - 1, sample_size, dtype=int)
                real_v_sampled = real_v_norm[indices]
                synth_v_sampled = synth_v_norm[indices]
            else:
                real_v_sampled = real_v_norm
                synth_v_sampled = synth_v_norm
            
            # ä½¿ç”¨é™æ¡æ¨£å¾Œçš„çŸ­åºåˆ—é€²è¡ŒDTWè¨ˆç®—ï¼Œä¸éœ€ window åƒæ•¸
            alignment = dtw(real_v_sampled, synth_v_sampled, keep_internals=True)
            
            dtw_dist = alignment.distance
            # ä½¿ç”¨æ¡æ¨£å¾Œåºåˆ—çš„é•·åº¦é€²è¡Œæ­£è¦åŒ–
            dtw_score = max(0, 1 - dtw_dist / len(real_v_sampled))

            # ç¶œåˆæ™‚é–“åˆ†æ•¸ (ACF:40%, FFT:30%, DTW:30%)
            time_score = (acf_score * 0.4 + fft_score * 0.3 + dtw_score * 0.3) * 100
            time_grade = "âœ…å„ªç§€" if time_score >= 85 else "âš ï¸è‰¯å¥½" if time_score >= 70 else "âŒéœ€æ”¹å–„"
            
            time_results.append({
                'ç‰¹å¾µ': feature, 'ACFå·®ç•°': f"{acf_diff:.4f}", 'FFTå·®ç•°': f"{fft_diff:.4f}",
                'DTWè·é›¢': f"{dtw_dist:.4f}", 'ç¶œåˆæ™‚é–“åˆ†æ•¸': f"{time_score:.1f}", 'æ™‚åºç›¸ä¼¼æ€§': time_grade
            })
        
        if time_results:
            print(tabulate(time_results, headers='keys', tablefmt='grid'))
            self.results['time_series_analysis'] = pd.DataFrame(time_results)
        else:
            print("æ²’æœ‰æ‰¾åˆ°å¯ä¾›åˆ†æçš„æ™‚é–“åºåˆ—ç‰¹å¾µã€‚")
            self.results['time_series_analysis'] = pd.DataFrame()
            
        print("\n")

    def _utility_assessment(self):
        """6. æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§è©•ä¼° (TSTR)"""
        print("ğŸ”§ 6. æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§è©•ä¼° (TSTR)")
        print("-" * 60)
        
        target = 'PHVAC_y'
        if target not in self.numeric_columns:
            print(f"ç›®æ¨™è®Šæ•¸ '{target}' ä¸å­˜åœ¨ï¼Œè·³é TSTR è©•ä¼°ã€‚")
            self.results['utility_assessment'] = {'score': 0, 'r2': -999}
            return

        features = [col for col in self.numeric_columns if col != target]
        
        X_train_synth = self.synthetic_data[features]
        y_train_synth = self.synthetic_data[target]
        X_test_real = self.real_data[features]
        y_test_real = self.real_data[target]
        
        model = RandomForestRegressor(n_estimators=20, max_depth=10, random_state=42, n_jobs=-1)
        model.fit(X_train_synth, y_train_synth)
        
        predictions = model.predict(X_test_real)
        r2 = r2_score(y_test_real, predictions)
        
        # å°‡ R2 åˆ†æ•¸è½‰æ›ç‚º 0-100 çš„å¯¦ç”¨æ€§åˆ†æ•¸
        utility_score = max(0, r2) * 100
        
        utility_grade = "âœ…å„ªç§€" if utility_score > 70 else "âš ï¸è‰¯å¥½" if utility_score > 50 else "âŒéœ€æ”¹å–„"
        
        print(f"åœ¨ä»¿çœŸè³‡æ–™ä¸Šè¨“ç·´ï¼Œåœ¨çœŸå¯¦è³‡æ–™ä¸Šæ¸¬è©¦çš„ RÂ² åˆ†æ•¸: {r2:.4f}")
        print(f"å¯¦ç”¨æ€§åˆ†æ•¸: {utility_score:.1f} / 100")
        print(f"å¯¦ç”¨æ€§ç­‰ç´š: {utility_grade}")
        
        self.results['utility_assessment'] = {'score': utility_score, 'r2': r2}
        print("\n")

    def _multivariate_analysis(self):
        """7. å¤šè®Šé‡çµæ§‹è©•ä¼° (PCA-based)"""
        print("ğŸŒ 7. å¤šè®Šé‡çµæ§‹è©•ä¼° (PCA)")
        print("-" * 60)
        
        features = self.numeric_columns
        
        scaler = StandardScaler()
        real_scaled = scaler.fit_transform(self.real_data[features])
        synth_scaled = scaler.transform(self.synthetic_data[features])
        
        pca = PCA(n_components=min(len(features), 5)) # æœ€å¤šå–5å€‹ä¸»æˆåˆ†
        pca.fit(real_scaled)
        
        real_pca = pca.transform(real_scaled)
        synth_pca = pca.transform(synth_scaled)
        
        pca_results = []
        distances = []
        for i in range(pca.n_components_):
            dist = wasserstein_distance(real_pca[:, i], synth_pca[:, i])
            distances.append(dist)
            pca_results.append({
                'ä¸»æˆåˆ†': f"PC-{i+1}",
                'è§£é‡‹è®Šç•°æ¯”ä¾‹': f"{pca.explained_variance_ratio_[i]:.2%}",
                'åˆ†ä½ˆè·é›¢(WD)': f"{dist:.4f}"
            })
            
        df_pca_results = pd.DataFrame(pca_results) # å°‡çµæœè½‰ç‚º DataFrame
        print(tabulate(pca_results, headers='keys', tablefmt='grid'))
        
        # å°‡å¹³å‡è·é›¢è½‰æ›ç‚ºåˆ†æ•¸ (0-100)
        avg_dist = np.mean(distances)
        # å‡è¨­å¹³å‡è·é›¢ > 0.5 æ„å‘³è‘—çµæ§‹å·®ç•°å¾ˆå¤§
        multivar_score = max(0, 100 * (1 - avg_dist / 0.5)) 
        multivar_grade = "âœ…å„ªç§€" if multivar_score > 85 else "âš ï¸è‰¯å¥½" if multivar_score > 70 else "âŒéœ€æ”¹å–„"
        
        print(f"\nå¹³å‡ä¸»æˆåˆ†åˆ†ä½ˆè·é›¢: {avg_dist:.4f}")
        print(f"å¤šè®Šé‡çµæ§‹åˆ†æ•¸: {multivar_score:.1f} / 100")
        print(f"çµæ§‹ç›¸ä¼¼åº¦: {multivar_grade}")
        
        # å°‡è©³ç´°è¡¨æ ¼ä¹Ÿå­˜å…¥ results
        self.results['multivariate_analysis'] = {'score': multivar_score, 'avg_dist': avg_dist, 'details': df_pca_results}
        print("\n")
        
    def _calculate_overall_scores(self):
        """8. è¨ˆç®—ç¶œåˆè©•åˆ† (å·²æ›´æ–°æ¬Šé‡èˆ‡ç¶­åº¦)"""
        print("ğŸ† 8. ç¶œåˆè©•åˆ†è¨ˆç®—")
        print("-" * 60)
        
        scores = {}
        # çµ±è¨ˆæº–ç¢ºæ€§
        mean_errors = self.results['descriptive_stats']['å‡å€¼èª¤å·®(%)'].str.replace('%', '').astype(float)
        avg_error = mean_errors.mean()
        scores['çµ±è¨ˆæº–ç¢ºæ€§'] = max(0, 100 - avg_error * 4)

        # åˆ†ä½ˆç›¸ä¼¼æ€§
        dist_scores = []
        stats_df = self.results['descriptive_stats'].set_index('ç‰¹å¾µ')
        for _, row in self.results['fidelity_metrics'].iterrows():
            w_dist = float(row['Wassersteinè·é›¢'])
            real_std = float(stats_df.loc[row['ç‰¹å¾µ'], 'çœŸå¯¦æ¨™æº–å·®'])
            if real_std > 1e-9:
                normalized_dist = w_dist / real_std
                dist_scores.append(max(0, 100 * (1 - normalized_dist)))
        scores['åˆ†ä½ˆç›¸ä¼¼æ€§'] = np.mean(dist_scores) if dist_scores else 0

        # HVACé‚è¼¯
        hvac_score = 100
        if 'hvac_specific' in self.results and self.results['hvac_specific'] is not None:
             for _, row in self.results['hvac_specific'].iterrows():
                 if 'âŒ' in str(row['ç‹€æ…‹']): hvac_score -= 30
                 elif 'âš ï¸' in str(row['ç‹€æ…‹']): hvac_score -= 15
        scores['HVACé‚è¼¯'] = max(0, hvac_score)

        # æ™‚é–“ç‰¹æ€§
        scores['æ™‚é–“ç‰¹æ€§'] = self.results['time_series_analysis']['ç¶œåˆæ™‚é–“åˆ†æ•¸'].astype(float).mean()

        # ç›¸é—œæ€§çµæ§‹
        scores['ç›¸é—œæ€§çµæ§‹'] = max(0, 100 * (1 - self.mean_corr_diff / 0.3))

        # å¯¦ç”¨æ€§ (TSTR)
        scores['æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§'] = self.results['utility_assessment']['score']

        # å¤šè®Šé‡çµæ§‹
        scores['å¤šè®Šé‡çµæ§‹'] = self.results['multivariate_analysis']['score']

        # æ›´æ–°å¾Œçš„æ¬Šé‡
        weights = {
            'çµ±è¨ˆæº–ç¢ºæ€§': 0.10, 'åˆ†ä½ˆç›¸ä¼¼æ€§': 0.15, 'HVACé‚è¼¯': 0.20,
            'æ™‚é–“ç‰¹æ€§': 0.15, 'ç›¸é—œæ€§çµæ§‹': 0.10, 'å¤šè®Šé‡çµæ§‹': 0.15,
            'æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§': 0.15
        }
        
        total_score = sum(scores[key] * weights[key] for key in scores)
        grade = "A+(å“è¶Š)" if total_score >= 90 else "A(å„ªç§€)" if total_score >= 80 else "B(è‰¯å¥½)" if total_score >= 70 else "C(åˆæ ¼)" if total_score >= 60 else "D(éœ€æ”¹å–„)"

        score_table = [{'è©•ä¼°ç¶­åº¦': dim, 'åˆ†æ•¸': f"{score:.1f}", 'æ¬Šé‡': f"{weights[dim]:.0%}", 'åŠ æ¬Šåˆ†æ•¸': f"{score * weights[dim]:.1f}"} for dim, score in scores.items()]
        
        print(tabulate(score_table, headers='keys', tablefmt='grid'))
        print(f"\n{'='*40}\nğŸ“Š ç¶œåˆè©•åˆ†: {total_score:.1f} / 100\nğŸ–ï¸ å“è³ªç­‰ç´š: {grade}\n{'='*40}")
        
        self.results['overall_scores'] = {'scores': scores, 'weights': weights, 'total_score': total_score, 'grade': grade}
        print("\n")
        
    # --- è¼”åŠ©å‡½å¼èˆ‡è¦–è¦ºåŒ– (ç‚ºç°¡æ½”çœç•¥å¤§éƒ¨åˆ†ï¼Œåƒ…å±•ç¤ºå¿…è¦ä¿®æ”¹) ---

    def _calculate_fft_diff(self, s1, s2, n_peaks=5):
        """è¨ˆç®—FFTå·®ç•° (å¾åŸ _calculate_tc_score æ”¹å)"""
        min_len = min(len(s1), len(s2))
        s1, s2 = s1[:min_len], s2[:min_len]
        amp1, amp2 = np.abs(np.fft.fft(s1))[:min_len//2], np.abs(np.fft.fft(s2))[:min_len//2]
        peaks1, peaks2 = np.argsort(amp1[1:])[-n_peaks:]+1, np.argsort(amp2[1:])[-n_peaks:]+1
        all_peaks = np.union1d(peaks1, peaks2)
        norm_amp1, norm_amp2 = amp1/(np.sum(amp1)+1e-10), amp2/(np.sum(amp2)+1e-10)
        return np.mean(np.abs(norm_amp1[all_peaks] - norm_amp2[all_peaks]))

    def _generate_visualizations(self):
        """ç”Ÿæˆæ‰€æœ‰å¯è¦–åŒ–åœ–è¡¨"""
        print("ğŸ¨ 9. ç”Ÿæˆå¯è¦–åŒ–åˆ†æåœ–è¡¨")
        print("-" * 60)
        vis_dir = os.path.join(self.output_dir, 'visualizations')
        os.makedirs(vis_dir, exist_ok=True)
        # (é€™è£¡æ‡‰åŒ…å«æ‰€æœ‰ç¹ªåœ–å‡½å¼çš„å‘¼å«)
        self._plot_radar_chart(vis_dir) # é›·é”åœ–éœ€è¦æ›´æ–°
        # ... å…¶ä»–ç¹ªåœ–å‡½å¼ ...
        print(f"âœ… æ‰€æœ‰åœ–è¡¨å·²ä¿å­˜è‡³: {vis_dir}\n")

    def _plot_radar_chart(self, output_dir):
        """ç¹ªè£½ç¶œåˆè©•åˆ†é›·é”åœ– (å·²æ›´æ–°ç¶­åº¦)"""
        if 'overall_scores' not in self.results: return
            
        scores_data = self.results['overall_scores']
        categories = list(scores_data['scores'].keys())
        values = list(scores_data['scores'].values())
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        ax.plot(angles, values, 'o-', linewidth=2, color='red', label='å¯¦éš›å¾—åˆ†')
        ax.fill(angles, values, alpha=0.25, color='red')
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, size=12)
        ax.set_ylim(0, 100)
        ax.set_title(f"ç¶œåˆå“è³ªè©•ä¼°é›·é”åœ–\nç¸½åˆ†: {scores_data['total_score']:.1f}/100 ({scores_data['grade']})", 
                     size=16, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'quality_radar.png'), dpi=300)
        plt.close()

    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç¶œåˆæ–‡å­—å ±å‘Š (å·²æ›´æ–°)"""
        # --- ğŸ’¥ é—œéµä¿®æ”¹ï¼ ğŸ’¥ ---
        # åŸæœ¬çš„è·¯å¾‘æ˜¯å¯«æ­»çš„ï¼Œç¾åœ¨æˆ‘å€‘è®“å®ƒå‹•æ…‹ç”Ÿæˆåœ¨æŒ‡å®šçš„ output_dir ä¸­
        # é€™æ¨£å ±å‘Šå°±æœƒè·Ÿåœ–è¡¨ä¸€èµ·å„²å­˜åœ¨æ­£ç¢ºçš„ä½ç½®
        report_path = os.path.join(self.output_dir, 'comprehensive_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            # ... (å¾Œé¢æ‰€æœ‰å¯«å…¥æª”æ¡ˆçš„ f.write(...) å…§å®¹éƒ½å®Œå…¨ä¸è®Š) ...
            f.write("="*80 + "\nHVAC ä»¿çœŸè³‡æ–™é›†ç¶œåˆè©•ä¼°å ±å‘Š (å„ªåŒ–ç‰ˆ)\n" + "="*80 + "\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"çœŸå¯¦è³‡æ–™: {self.real_data_file}\n")
            f.write(f"ä»¿çœŸè³‡æ–™: {self.synthetic_data_file}\n\n")

            # --- 1. ç¸½é«”è©•ä¼° ---
            if 'overall_scores' in self.results:
                scores_data = self.results['overall_scores']
                f.write("--- ğŸ† ç¸½é«”è©•ä¼° ---\n")
                f.write(f"ç¶œåˆè©•åˆ†: {scores_data['total_score']:.1f}/100\n")
                f.write(f"å“è³ªç­‰ç´š: {scores_data['grade']}\n\n")
                f.write("å„ç¶­åº¦å¾—åˆ†:\n")
                df_scores = pd.DataFrame({
                    'è©•ä¼°ç¶­åº¦': scores_data['scores'].keys(),
                    'åˆ†æ•¸': [f"{s:.1f}" for s in scores_data['scores'].values()],
                    'æ¬Šé‡': [f"{w:.0%}" for w in scores_data['weights'].values()],
                    'åŠ æ¬Šåˆ†æ•¸': [f"{s * w:.1f}" for s, w in zip(scores_data['scores'].values(), scores_data['weights'].values())]
                })
                f.write(df_scores.to_string(index=False))
                f.write("\n\n" + "="*80 + "\n\n")
            
            f.write("--- ğŸ“œ è©³ç´°åˆ†æçµæœ ---\n\n")

            # --- 2. æè¿°æ€§çµ±è¨ˆåˆ†æ ---
            if 'descriptive_stats' in self.results and not self.results['descriptive_stats'].empty:
                f.write("--- ğŸ“ˆ 1. æè¿°æ€§çµ±è¨ˆåˆ†æ ---\n")
                f.write(self.results['descriptive_stats'].to_string(index=False))
                f.write("\n\n")

            # --- 3. åˆ†ä½ˆä¿çœŸåº¦è©•ä¼° ---
            if 'fidelity_metrics' in self.results and not self.results['fidelity_metrics'].empty:
                f.write("--- ğŸ¯ 2. åˆ†ä½ˆä¿çœŸåº¦è©•ä¼° (Fidelity Assessment) ---\n")
                f.write(self.results['fidelity_metrics'].to_string(index=False))
                f.write("\n\n")
                
            # --- 4. HVACç³»çµ±å°ˆå±¬åˆ†æ ---
            if 'hvac_specific' in self.results and not self.results['hvac_specific'].empty:
                f.write("--- ğŸ­ 3. HVACç³»çµ±å°ˆå±¬åˆ†æ ---\n")
                f.write(self.results['hvac_specific'].to_string(index=False))
                f.write("\n\n")

            # --- 5. ç›¸é—œæ€§çµæ§‹åˆ†æ ---
            if 'correlation_analysis' in self.results:
                corr_data = self.results['correlation_analysis']
                f.write("--- ğŸ”— 4. ç›¸é—œæ€§çµæ§‹åˆ†æ ---\n")
                f.write(f"å¹³å‡ç›¸é—œæ€§å·®ç•°: {corr_data['mean_diff']:.4f}\n")
                f.write(f"ç›¸é—œæ€§ä¿æŒç¨‹åº¦: {corr_data['grade']}\n\n")

            # --- 6. æ™‚é–“åºåˆ—ç‰¹æ€§åˆ†æ ---
            if 'time_series_analysis' in self.results and not self.results['time_series_analysis'].empty:
                f.write("--- â° 5. æ™‚é–“åºåˆ—ç‰¹æ€§åˆ†æ ---\n")
                f.write(self.results['time_series_analysis'].to_string(index=False))
                f.write("\n\n")
            
            # --- 7. æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§è©•ä¼° (TSTR) ---
            if 'utility_assessment' in self.results:
                utility_data = self.results['utility_assessment']
                f.write("--- ğŸ”§ 6. æ©Ÿå™¨å­¸ç¿’å¯¦ç”¨æ€§è©•ä¼° (TSTR) ---\n")
                f.write(f"åœ¨ä»¿çœŸè³‡æ–™ä¸Šè¨“ç·´ï¼Œåœ¨çœŸå¯¦è³‡æ–™ä¸Šæ¸¬è©¦çš„ RÂ² åˆ†æ•¸: {utility_data['r2']:.4f}\n")
                f.write(f"å¯¦ç”¨æ€§åˆ†æ•¸: {utility_data['score']:.1f} / 100\n\n")

            # --- 8. å¤šè®Šé‡çµæ§‹è©•ä¼° (PCA) ---
            if 'multivariate_analysis' in self.results:
                multi_data = self.results['multivariate_analysis']
                f.write("--- ğŸŒ 7. å¤šè®Šé‡çµæ§‹è©•ä¼° (PCA) ---\n")
                if 'details' in multi_data and not multi_data['details'].empty:
                    f.write("ä¸»æˆåˆ†åˆ†ä½ˆæ¯”è¼ƒ:\n")
                    f.write(multi_data['details'].to_string(index=False) + "\n\n")
                f.write(f"å¹³å‡ä¸»æˆåˆ†åˆ†ä½ˆè·é›¢: {multi_data['avg_dist']:.4f}\n")
                f.write(f"å¤šè®Šé‡çµæ§‹åˆ†æ•¸: {multi_data['score']:.1f} / 100\n\n")

        print(f"ğŸ“„ ç¶œåˆå ±å‘Šå·²ç”Ÿæˆ (åŒ…å«è©³ç´°è³‡è¨Š): {report_path}")

# --- ä¸»ç¨‹å¼å…¥å£ ---
if __name__ == "__main__":
    # --- é€™å€‹å€å¡Šç¾åœ¨åƒ…ä¾›ç¨ç«‹æ¸¬è©¦ä½¿ç”¨ ---
    print(">> è­¦å‘Šï¼šæ­¤è…³æœ¬è¢«ç¨ç«‹åŸ·è¡Œï¼Œåƒ…ç”¨æ–¼æ¸¬è©¦ç›®çš„ <<")
    
    # é€™è£¡ä»ç„¶å¯ä»¥ä½¿ç”¨å¯«æ­»çš„è·¯å¾‘ä¾†å¿«é€Ÿæ¸¬è©¦é€™å€‹è…³æœ¬çš„åŠŸèƒ½
    real_data_path = 'data/ChillerFinal500Ping_res.csv'
    synth_data_path = './logs/20250922-223235_ä¹Ÿé‚„ä¸éŒ¯/hvac_rl_gan_generated.csv' # æ¸¬è©¦æ™‚è«‹ç¢ºä¿æ­¤æª”æ¡ˆå­˜åœ¨

    if os.path.exists(real_data_path) and os.path.exists(synth_data_path):
        print("æª¢æ¸¬åˆ°è³‡æ–™æª”æ¡ˆï¼Œé–‹å§‹åŸ·è¡Œå„ªåŒ–ç‰ˆåˆ†æ...\n")
        analyzer = HVACComprehensiveAnalyzer(real_data_path, synth_data_path)
        # æ¸¬è©¦æ™‚ï¼Œçµæœæœƒå­˜åœ¨ä¸€å€‹åç‚º "analysis_results_test" çš„è³‡æ–™å¤¾
        analyzer.comprehensive_analysis(output_dir="analysis_results_test") 
    else:
        print(f"âŒ éŒ¯èª¤ï¼šè«‹ç¢ºä¿ '{real_data_path}' å’Œ '{synth_data_path}' æª”æ¡ˆå­˜åœ¨æ–¼ç•¶å‰ç›®éŒ„ã€‚")